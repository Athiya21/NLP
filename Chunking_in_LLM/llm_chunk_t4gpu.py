# -*- coding: utf-8 -*-
"""LLM_Chunk_T4GPU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SyAwnOxmDTyPn9xQ3D0NZ1sr9LS8RyKh
"""

from google.colab import drive
drive.mount('/content/drive')

!nvidia-smi

!pip install transformers

from transformers import AutoTokenizer

# Load the tokenizer for a specific model (e.g., GPT-2)
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# Tokenize some input text
text = "Hello, how are you?"
tokens = tokenizer(text, return_tensors='pt')
print(tokens)

"""- input text "Hello, how are you?". Specifically, the output will be a dictionary containing the token IDs and attention masks in a format that PyTorch models can use.

- Token IDs: A tensor containing the integer representations of the tokens from the input text.

- Attention Mask: A tensor indicating which tokens should be attended to (1 for real tokens, 0 for padding).

- input_ids: This is a tensor containing the numerical IDs corresponding to the tokens. For the input text, it may look like [15496, 11, 703, 389, 345, 329, 30]. Each number corresponds to a specific token in the GPT-2 vocabulary.

- attention_mask: This tensor is used to indicate which tokens should be processed by the model. In this case, since there are no padding tokens, all values are 1.
"""

from transformers import AutoModelForCausalLM

# Load the pre-trained GPT-2 model
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Generate text
input_ids = tokenizer.encode("indian cricket", return_tensors='pt')
output = model.generate(input_ids, max_length=50)
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

print(generated_text)

"""- When you run the provided code, it uses the GPT-2 model to generate text based on the prompt "Once upon a time." Hereâ€™s a breakdown of what happens and what you can expect as output:

- Steps in the Code
Load the Model: The AutoModelForCausalLM.from_pretrained("gpt2") line loads the pre-trained GPT-2 model.

Tokenization: The prompt "Once upon a time" is tokenized into input IDs that the model can understand.

- Text Generation: The model.generate() method generates text based on the input IDs. The max_length=50 argument specifies that the total length of the generated text (including the prompt) should not exceed 50 tokens.

- Decoding: The output is then decoded back into human-readable text using the tokenizer.

#Output Characteristics
- Length: The length of the output will depend on the prompt and the max_length parameter. If the prompt is short and max_length is set to 50, the output will be roughly 30 to 40 tokens of generated text.

- Creativity: The continuation may include imaginative scenarios, characters, or events that align with the narrative style of fairy tales or stories.
"""

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load a pre-trained model and tokenizer
model_name = "gpt2"  # You can replace with any other LLM
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def chunk_text(text, max_length=512):
    """Chunk text into smaller pieces."""
    tokens = tokenizer.encode(text, return_tensors='pt')[0]
    chunks = []

    for i in range(0, len(tokens), max_length):
        chunk = tokens[i:i + max_length]
        chunks.append(chunk)

    return chunks

def generate_responses(chunks):
    """Generate responses for each chunk using the LLM."""
    responses = []
    for chunk in chunks:
        input_ids = chunk.unsqueeze(0)  # Add batch dimension
        # Increase max_length to a value greater than or equal to the longest chunk length
        output = model.generate(input_ids, max_length=512)  # Generate response
        responses.append(tokenizer.decode(output[0], skip_special_tokens=True))

    return responses

# Example long text
long_text = "india " * 5  # Repeat to simulate long text

# Chunk the text
chunks = chunk_text(long_text)

# Generate responses for each chunk
responses = generate_responses(chunks)

# Print the responses
for i, response in enumerate(responses):
    print(f"Response for chunk {i+1}:\n{response}\n")

"""- AutoTokenizer and AutoModelForCausalLM are part of the Hugging Face transformers library, which simplifies the process of working with various pre-trained transformer models.

- AutoTokenizer
Purpose: AutoTokenizer is designed to automatically retrieve the appropriate tokenizer for a given model. Tokenizers convert raw text into tokens that the model can understand, and they also handle various tasks like adding special tokens, padding, and truncating.

- Usage:
You can load a tokenizer by specifying the model name or path.
The tokenizer will be automatically configured according to the model's requirements.

- Explanation of the Code

- Loading the Model and Tokenizer:

- The code loads a pre-trained GPT-2 model and its corresponding tokenizer. You can replace "gpt2" with any other compatible model.

- Chunking Function:
The chunk_text function takes a string of text and splits it into chunks of a specified maximum length (in tokens). It encodes the text into tokens and then slices it into manageable pieces.

- Generating Responses:
The generate_responses function iterates through each chunk, generates a response using the model, and decodes the output back into text.

- Putting It All Together:
A long text is created (you can replace this with your actual text).
The text is chunked, and responses are generated for each chunk.

- Output
The output will show responses generated for each chunk of the input text, allowing you to process longer texts effectively without exceeding the token limit of the model.

- Note
When processing multiple chunks, consider how to handle overlapping content, especially if the chunks are related, to maintain context. You might want to implement strategies like including the last few tokens of the previous chunk in the next one.
"""

from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')
set_seed(42)
generator("Hello, I'm a language model,", max_length=30, num_return_sequences=5)

from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')
set_seed(42)
generator("Hello, explain about indian economy ,", max_length=10, num_return_sequences=2)

from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
print(output)

# Tensorflow vectorization code

from transformers import GPT2Tokenizer, TFGPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = TFGPT2Model.from_pretrained('gpt2')
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
print(output)

from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')

set_seed(42)
generator("The White man worked as a", max_length=10, num_return_sequences=5)


set_seed(42)
generator("The Black man worked as a", max_length=10, num_return_sequences=5)