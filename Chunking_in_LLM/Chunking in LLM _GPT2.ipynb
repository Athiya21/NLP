{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For chunking text to process with large language models (LLMs), you can create a simple function that splits long texts into smaller chunks.\n",
    "\n",
    "- This is particularly useful when the input text exceeds the model's token limit. Below is an example using Python, leveraging the transformers library to illustrate how you can chunk text before feeding it into an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\fathima\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: requests in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.13.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2025.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\fathima\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from torch) (2.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: sympy in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from torch) (1.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from networkx->torch) (4.4.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\fathima\\anaconda3\\lib\\site-packages (from sympy->torch) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for chunk 1:\n",
      "brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai Â explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load a pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # You can replace with any other LLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def chunk_text(text, max_length=512):\n",
    "    \"\"\"Chunk text into smaller pieces.\"\"\"\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')[0]\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk = tokens[i:i + max_length]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def generate_responses(chunks):\n",
    "    \"\"\"Generate responses for each chunk using the LLM.\"\"\"\n",
    "    responses = []\n",
    "    for chunk in chunks:\n",
    "        input_ids = chunk.unsqueeze(0)  # Add batch dimension\n",
    "        # Increase max_length to a value greater than or equal to the longest chunk length\n",
    "        output = model.generate(input_ids, max_length=512)  # Generate response\n",
    "        responses.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "    return responses\n",
    "\n",
    "# Example long text\n",
    "long_text = \"brief explain about generative ai \" * 50  # Repeat to simulate long text\n",
    "\n",
    "# Chunk the text\n",
    "chunks = chunk_text(long_text)\n",
    "\n",
    "# Generate responses for each chunk\n",
    "responses = generate_responses(chunks)\n",
    "\n",
    "# Print the responses\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Response for chunk {i+1}:\\n{response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
